import requests
import pytest
import time
import json
from os.path import join
from article_metrics import utils
import responses
from unittest.mock import patch
from . import base
from article_metrics.scopus import citations
from django.conf import settings
from unittest import skip

def test_scopus_parse_entry():
    "citations.parse_entry can handle all known fixtures"
    response_fixtures = [
        base.fixture_json('scopus-responses/search-p1.json'),
        base.fixture_json('scopus-responses/search-p2.json')
    ]
    results = citations.all_entries(response_fixtures)
    per_page = 25
    expected = per_page * len(response_fixtures)
    assert expected == len(results)

def test_scopus_parse_entry_multiple_dois():
    fixture = base.fixture_json("scopus-doi-as-list.json")
    entry = fixture['data']
    result = citations.parse_entry(entry)
    expected = "10.7554/eLife.00387"
    assert expected == result['doi']

def test_scopus_unparseable_entry():
    "unparseable entries have a different return type"
    entry_list = base.fixture_json('scopus-responses/search-p1.json')
    good_entry = entry_list['entry'][0]
    with patch('article_metrics.handler.writefile') as mk:
        entry = good_entry
        # skitch entry so it's bad
        del entry['prism:doi']
        result = citations.parse_entry(entry)
        assert 'bad' in result  # ll: {'bad': ...}
        # a bad entry is still returned but the body isn't written to disk for inspection
        assert not mk.called

def test_scopus_data_dumps():
    """similar to `test_scopus_parse_entry` but geared for data dumps generated by handler.writefile.
    these dumps are slightly different from raw scopus results pages"""
    response_fixtures = utils.listfiles(join(base.FIXTURE_DIR, 'scopus-responses', 'dumps'))
    for response in response_fixtures:
        with patch('article_metrics.handler.capture_parse_error', return_value=lambda fn: fn):
            try:
                fixture = json.load(open(response, 'r'))['data']
                utils.lmap(citations.parse_entry, fixture)
            except BaseException as err:
                print('caught error in', response)
                raise err

# TODO: this is talking to scopus.
# see fixtures/scopus-responses/search-p1.json and p2.json
@skip('this is talking to scopus')
def test_scopus_request():
    search_gen = citations.search(settings.SCOPUS_KEY, settings.DOI_PREFIX)
    search_results = next(search_gen)
    assert 'opensearch:totalResults' in search_results
    expected = '0'
    assert expected == search_results['opensearch:startIndex']

    search_results2 = next(search_gen)
    assert 'opensearch:totalResults' in search_results
    expected = '1'
    assert expected == search_results2['opensearch:startIndex']

def test_scopus_search_exits_early():
    "we stop talking to scopus when they start returning results with 0 citations"
    pass

def test_scopus_search():

    def obj(method, y):
        return type('mock', (object,), {method: lambda: y})

    # pagination request
    response1 = obj('json', {'search-results': {'opensearch:totalResults': 2, 'entry': []}})
    response2 = obj('json', {'search-results': base.fixture_json('scopus-responses/dodgy-scopus-results.json')})

    with patch('article_metrics.scopus.citations.fetch_page', side_effect=[response1, response2]):
        # we get something for all of our entries
        results = citations.all_todays_entries()
        total_results = 26
        assert total_results == len(results)

        unparseable_entries = 3
        unknown_doi_prefixes = 1
        subresource_dois = 2
        bad_eggs = unparseable_entries + unknown_doi_prefixes + subresource_dois

        expected = len([r for r in results if 'bad' not in r])
        assert total_results - bad_eggs == expected

@responses.activate
def test_scopus_search__initial_failure():
    "failure to fetch the first page of results cancels operation"
    response = requests.exceptions.RetryError()
    responses.add(responses.GET, citations.URL, body=response)
    with pytest.raises(AssertionError):
        list(citations.search())

@responses.activate
def test_scopus_search__failure():
    "failure to fetch a page of results skips result"
    response1 = {'search-results': {'opensearch:totalResults': 2, 'entry': []}}
    response2 = requests.exceptions.RetryError()
    responses.add(responses.GET, citations.URL, json=response1)
    responses.add(responses.GET, citations.URL, body=response2)
    # only the initial response is returned
    expected = {'entry': [], 'opensearch:totalResults': 2}
    assert next(citations.search()) == expected

@responses.activate
def test_many_scopus_requests():
    "scopus isn't hit more than N times a second"

    # don't actually hit scopus
    responses.add(responses.GET, citations.URL, **{
        'body': '',
        'status': 200,
        'content_type': 'text/plain'})

    with patch('article_metrics.handler.requests_get') as mock:
        start = time.time()
        # attempt to do more per-second than allowed
        for i in range(0, citations.MAX_PER_SECOND + 1):
            citations.fetch_page(**{
                'api_key': 'foo',
                'doi_prefix': settings.DOI_PREFIX,
                'page': i
            })
        end = time.time()
        elapsed = end - start # seconds

        # doing N + 1 requests takes longer than 1 second
        assert elapsed > 1
        expected = citations.MAX_PER_SECOND + 1
        assert expected == len(mock.mock_calls)
